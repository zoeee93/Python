{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[![AnalyticsDojo](../fig/final-logo.png)](http://rpi.analyticsdojo.com)\n",
    "<center><h1>Introduction to API's with Python</h1></center>\n",
    "<center><h3><a href = 'http://rpi.analyticsdojo.com'>rpi.analyticsdojo.com</a></h3></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is adopted from [Mining the Social Web, 2nd Edition](http://bit.ly/16kGNyb)\n",
    "Copyright (c) 2013, Matthew A. Russell\n",
    "All rights reserved.\n",
    "\n",
    "This work is licensed under the [Simplified BSD License](https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition/blob/master/LICENSE.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you Begin #1\n",
    "If you are working locally, this exercise requires the twitter package, which unlike previous packages needs pip as an installer. You may have already done this.\n",
    "`!pip install twitter`\n",
    "\n",
    "If you get an error that pip is not available, you might have to install it.  See \n",
    "https://conda.io/docs/user-guide/tasks/manage-pkgs.html.\n",
    "\n",
    "The package should be available online at lab.analyticsdojo.com (nothing to Install)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see if it worked by importing the twitter package & some other things we will use.  \n",
    "from  twitter import *\n",
    "import datetime, traceback \n",
    "import json\n",
    "import time\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Before you Begin #2\n",
    "In the twitter directory you will see a configuration file called configsample.yaml.  We are going to store our Twitter keys in there.  These Twitter keys are used instead of passwords to programatically allow access to your \n",
    "\n",
    "1. Copy `configsample.yaml` to `config.yaml`.\n",
    "2. Create an ID on Twitter or login if you already have a Twitter account. \n",
    "3. Create a twitter app.  Go to [apps.twitter.com](https://apps.twitter.com) and click on create a new app.  You will then be able to access the needed fields for the config.yaml file. \n",
    "4. Update config.yaml to include appropriate Twitter keys.\n",
    "5. Update screen_names.csv to include the ids of interest.\n",
    "6. Download your config file to somewhere outside this directory just in case you update the repository it might be lost. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1.  Loading Authorization Data\n",
    "- Here we are going to store the authorization data in a .YAML file rather than directly in the notebook.  \n",
    "- We have also added `config.yaml` to the `.gitignore` file so we won't accidentally commit our sensitive data to the repository.\n",
    "- You should generally keep sensitive data out of all git repositories (public or private) but definitely Public. \n",
    "- If you ever accidentally commit data to a public repository you must consider it compromised.\n",
    "- A .yaml file is a common way to store configuration data, but it is not really secure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will import some required libraries.\n",
    "import sys \n",
    "import ruamel.yaml #A .yaml file \n",
    "#This is your configuration file. \n",
    "twitter_yaml='./twitter/config.yaml'\n",
    "with open(twitter_yaml, 'r') as yaml_t:\n",
    "    cf_t=ruamel.yaml.round_trip_load(yaml_t, preserve_quotes=True)\n",
    "\n",
    "\n",
    "#You can check your config was loaded by printing, but you should not commit this.\n",
    "#print(cf_t)\n",
    "cf_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Some Relevant Functions\n",
    "- We first will create a Twitter object we can used to authorize data.\n",
    "- Then we will get profiles.\n",
    "- Finally we will get some tweets.  \n",
    "\n",
    "**Don't worry about not understanding all the code.  Here we are pushing you us more complex functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_twitter_auth(cf_t):\n",
    "        \"\"\"Function to create a twitter object\n",
    "           Args: cf_t is configuration dictionary. \n",
    "           Returns: Twitter object.\n",
    "            \"\"\"\n",
    "        # When using twitter stream you must authorize.\n",
    "        # these tokens are necessary for user authentication\n",
    "        # create twitter API object\n",
    "\n",
    "        auth = OAuth(cf_t['access_token'], cf_t['access_token_secret'], cf_t['consumer_key'], cf_t['consumer_secret'])\n",
    "\n",
    "        try:\n",
    "            # create twitter API object\n",
    "            twitter = Twitter(auth = auth)\n",
    "        except TwitterHTTPError:\n",
    "            traceback.print_exc()\n",
    "            time.sleep(cf_t['sleep_interval'])\n",
    "        return twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profiles(twitter, names, cf_t):\n",
    "    \"\"\"Function write profiles to a file with the form *data-user-profiles.json*\n",
    "       Args: names is a list of names\n",
    "             cf_t is a list of twitter config\n",
    "       Returns: Nothing\n",
    "        \"\"\"\n",
    "    # file name for daily tracking\n",
    "    dt = datetime.datetime.now()\n",
    "    fn = cf_t['data']+'/profiles/'+dt.strftime('%Y-%m-%d-user-profiles.json')\n",
    "    with open(fn, 'w') as f:\n",
    "        for name in names:\n",
    "            print(\"Searching twitter for User profile: \", name)\n",
    "            try:\n",
    "                # create a subquery, looking up information about these users\n",
    "                # twitter API docs: https://dev.twitter.com/docs/api/1/get/users/lookup\n",
    "                profiles = twitter.users.lookup(screen_name = name)\n",
    "                sub_start_time = time.time()\n",
    "                for profile in profiles:\n",
    "                    print(\"User found. Total tweets:\", profile['statuses_count'])\n",
    "                    # now save user info\n",
    "                    f.write(json.dumps(profile))\n",
    "                    f.write(\"\\n\")\n",
    "                sub_elapsed_time = time.time() - sub_start_time;\n",
    "                if sub_elapsed_time < cf_t['sleep_interval']:\n",
    "                    time.sleep(cf_t['sleep_interval'] + 1 - sub_elapsed_time)\n",
    "            except TwitterHTTPError:\n",
    "                traceback.print_exc()\n",
    "                time.sleep(cf_t['sleep_interval'])\n",
    "                continue\n",
    "    f.close()\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Twitter Handle From CSV\n",
    "- This is a .csv that has individuals we want to collect data on. \n",
    "- Go ahead and follow [AnalyticsDojo](https://twitter.com/AnalyticsDojo).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(cf_t['config']+\"/\"+cf_t['file'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Twitter Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Twitter Object\n",
    "twitter= create_twitter_auth(cf_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will get general profile data\n",
    "profiles_fn=get_profiles(twitter, df['screen_name'], cf_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outcoming of running the above API is to generate a twitter object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Getting Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get some help on how to use the twitter api with the following. \n",
    "help(twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Go ahead and take a look at the [twitter docs](https://dev.twitter.com/rest/public).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Yahoo! Where On Earth ID for the entire world is 1.\n",
    "# See https://dev.twitter.com/docs/api/1.1/get/trends/place and\n",
    "# http://developer.yahoo.com/geo/geoplanet/\n",
    "\n",
    "WORLD_WOE_ID = 1\n",
    "US_WOE_ID = 23424977\n",
    "\n",
    "# Prefix ID with the underscore for query string parameterization.\n",
    "# Without the underscore, the twitter package appends the ID value\n",
    "# to the URL itself as a special case keyword argument.\n",
    "\n",
    "world_trends = twitter.trends.place(_id=WORLD_WOE_ID)\n",
    "us_trends = twitter.trends.place(_id=US_WOE_ID)\n",
    "\n",
    "print (world_trends)\n",
    "print (us_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Displaying API responses as pretty-printed JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print (json.dumps(world_trends, indent=1))\n",
    "print (json.dumps(us_trends, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the [api docs](https://dev.twitter.com/rest/reference/get/trends/place) for the /trends/place call made above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Collecting search results for a targeted hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import unquote to prevent url encoding errors in next_results\n",
    "#from urllib3 import unquote\n",
    "\n",
    "#This can be any trending topic, but let's focus on a hashtag that is relevant to the class. \n",
    "q = '#analytics' \n",
    "\n",
    "count = 100\n",
    "\n",
    "# See https://dev.twitter.com/rest/reference/get/search/tweets\n",
    "search_results = twitter.search.tweets(q=q, count=count)\n",
    "\n",
    "#This selects out \n",
    "statuses = search_results['statuses']\n",
    "\n",
    "\n",
    "# Iterate through 5 more batches of results by following the cursor\n",
    "for _ in range(5):\n",
    "    print (\"Length of statuses\", len(statuses))\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "        print (\"next_results\", next_results)\n",
    "    except: # No more results when next_results doesn't exist\n",
    "        break\n",
    "        \n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in next_results[1:].split(\"&\") ])\n",
    "    print (kwargs)\n",
    "    search_results = twitter.search.tweets(**kwargs)\n",
    "    statuses += search_results['statuses']\n",
    "\n",
    "# Show one sample search result by slicing the list...\n",
    "print (json.dumps(statuses[0], indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print several\n",
    "print (json.dumps(statuses[0:5], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Extracting text, screen names, and hashtags from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can access an individual tweet like so:\n",
    "statuses[1]['text']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses[1]['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice the nested relationships.  We have to take notice of this to further access the data.\n",
    "statuses[1]['entities']['hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_texts = [ status['text'] \n",
    "                 for status in statuses ]\n",
    "\n",
    "screen_names = [ user_mention['screen_name'] \n",
    "                 for status in statuses\n",
    "                     for user_mention in status['entities']['user_mentions'] ]\n",
    "\n",
    "hashtags = [ hashtag['text'] \n",
    "             for status in statuses\n",
    "                 for hashtag in status['entities']['hashtags'] ]\n",
    "\n",
    "urls = [ url['url'] \n",
    "             for status in statuses\n",
    "                 for url in status['entities']['urls'] ]\n",
    "\n",
    "\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [ w \n",
    "          for t in status_texts \n",
    "              for w in t.split() ]\n",
    "\n",
    "# Explore the first 5 items for each...\n",
    "\n",
    "print (json.dumps(status_texts[0:5], indent=1))\n",
    "print (json.dumps(screen_names[0:5], indent=1)) \n",
    "print (json.dumps(hashtags[0:5], indent=1))\n",
    "print (json.dumps(words[0:5], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Creating a basic frequency distribution from the words in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for item in [words, screen_names, hashtags]:\n",
    "    c = Counter(item)\n",
    "    print (c.most_common()[:10]) # top 10, \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
