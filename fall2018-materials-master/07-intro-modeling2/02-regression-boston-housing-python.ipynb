{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[![AnalyticsDojo](../fig/final-logo.png)](http://rpi.analyticsdojo.com)\n",
    "<center><h1>Introduction to Modeling - Origional Boston Housing </h1></center>\n",
    "<center><h3><a href = 'http://rpi.analyticsdojo.com'>rpi.analyticsdojo.com</a></h3></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This uses the same mechansims. \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview\n",
    "- Getting the Data\n",
    "- Reviewing Data\n",
    "- Modeling \n",
    "- Model Evaluation\n",
    "- Using Model\n",
    "- Storing Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Getting Data \n",
    "- Available in the [sklearn package](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) as a Bunch object (dictionary).\n",
    "- From FAQ: [\"Donâ€™t make a bunch object! They are not part of the scikit-learn API. Bunch objects are just a way to package some numpy arrays. As a scikit-learn user you only ever need numpy arrays to feed your model with data.\"](http://scikit-learn.org/stable/faq.html)\n",
    "- Available in the UCI data repository. \n",
    "- Better to convert to Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From sklearn tutorial.\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print( \"Type of boston dataset:\", type(boston))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A bunch is you remember is a dictionary based dataset.  Dictionaries are addressed by keys. \n",
    "#Let's look at the keys. \n",
    "print(boston.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#DESCR sounds like it could be useful. Let's print the description.\n",
    "print(boston['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's change the data to a Panda's Dataframe\n",
    "import pandas as pd\n",
    "boston_df = pd.DataFrame(boston['data'] )\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Now add the column names.\n",
    "boston_df.columns = boston['feature_names']\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Add the target as PRICE. \n",
    "boston_df['PRICE']= boston['target']\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ## Attribute Information (in order):\n",
    " Looks like they are all continuous IV and continuous DV. \n",
    "        - CRIM     per capita crime rate by town\n",
    "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        - INDUS    proportion of non-retail business acres per town\n",
    "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "        - RM       average number of rooms per dwelling\n",
    "        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "        - DIS      weighted distances to five Boston employment centres\n",
    "        - RAD      index of accessibility to radial highways\n",
    "        - TAX      full-value property-tax rate per 10,000\n",
    "        - PTRATIO  pupil-teacher ratio by town\n",
    "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        - LSTAT    % lower status of the population\n",
    "        - MEDV     Median value of owner-occupied homes in 1000's\n",
    " Let's check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#check for missing values\n",
    "print(np.sum(np.isnan(boston_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What type of data are there?\n",
    "- First let's focus on the dependent variable, as the nature of the DV is critical to selection of model. \n",
    "- *Median value of owner-occupied homes in $1000's* is the Dependent Variable  (continuous variable).\n",
    "- It is relevant to look at the distribution of the dependent variable, so let's do that first.\n",
    "- Here there is a normal distribution for the most part, with some at the top end of the distribution we could explore later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's us seaborn, because it is pretty. ;) \n",
    "#See more here. http://seaborn.pydata.org/tutorial/distributions.html\n",
    "import seaborn as sns\n",
    "sns.distplot(boston_df['PRICE']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#We can quickly look at other data. \n",
    "#Look at the bottom row to see thinks likely coorelated with price. \n",
    "#Look along the diagonal to see histograms of each. \n",
    "sns.pairplot(boston_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preparing to Model \n",
    "- It is common to separate `y` as the dependent variable and `X` as the matrix of independent variables.\n",
    "- Here we are using `train_test_split` to split the test and train.\n",
    "- This creates 4 subsets, with IV and DV separted: `X_train, X_test, y_train, y_test`\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will throw and error at import if haven't upgraded. \n",
    "# from sklearn.cross_validation  import train_test_split  \n",
    "from sklearn.model_selection  import train_test_split\n",
    "#y is the dependent variable.\n",
    "y = boston_df['PRICE']\n",
    "#As we know, iloc is used to slice the array by index number. Here this is the matrix of \n",
    "#independent variables.\n",
    "X = boston_df.iloc[:,0:13]\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Modeling \n",
    "- First import the package: `from sklearn.linear_model import LinearRegression`\n",
    "- Then create the model object.  \n",
    "- Then fit the data. \n",
    "- This creates a trained model (an object) of class regression.\n",
    "- The variety of methods and attributes available for regression are shown [here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit( X_train, y_train )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluating the Model Results\n",
    "- You have fit a model. \n",
    "- You can now store this model, save the object to disk, or evaluate it with different outcomes. \n",
    "- Trained regression objects have coefficients (`coef_`) and intercepts (`intercept_`) as attributes. \n",
    "- R-Squared is determined from the `score` method of the regression object.\n",
    "- For Regression, we are going to use the coefficient of determination as our way of evaluating the results, [also referred to as R-Squared](https://en.wikipedia.org/wiki/Coefficient_of_determination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('labels\\n',X.columns)\n",
    "print('Coefficients: \\n', lm.coef_)\n",
    "print('Intercept: \\n', lm.intercept_)\n",
    "print('R2 for Train)', lm.score( X_train, y_train ))\n",
    "print('R2 for Test (cross validation)', lm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Alternately, we can show the results in a dataframe using the zip command.\n",
    "pd.DataFrame( list(zip(X.columns, lm.coef_)),\n",
    "            columns=['features', 'estimatedCoeffs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross Validation and Hyperparameter Tuning\n",
    "- The basic way of having a train and a test set can result in overfitting if there are parameters within the model that are being optimized. [Further described here](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation).  \n",
    "- Because of this, a third validation set can be partitioned, but at times there isn't enough data.\n",
    "- So Cross validation can split the data into (`cv`) different datasets and check results. \n",
    "- Returning MSE rather than R2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(lm, X_train, y_train, cv=8) \n",
    "print(\"R2:\", scores, \"\\n R2_avg: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Calculation of Null Model \n",
    "- We also want to compare a null model (baseline model) with our result.  \n",
    "- To do this, we have to generate an array of equal size to the train and test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we need to constructor our Base model \n",
    "#This syntax multiplies a list by a number, genarating a list of length equal to that number.\n",
    "#Then we can cast it as a Pandas series.\n",
    "y_train_base = pd.Series([np.mean(y_train)] * y_train.size)\n",
    "y_test_base = pd.Series([np.mean(y_train)] * y_test.size)\n",
    "print(y_train_base.head(), '\\n Size:', y_train_base.size)\n",
    "print(y_test_base.head(), '\\n Size:', y_test_base.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scoring of Null Model\n",
    "- While previously we generated the R2 score from the `fit` method, passing X and Y, we can also score the r2 using the `r2_score` method, which is imported from sklearn.metrix.\n",
    "- The `r2_score` method accepts that true value and the predicted value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_train_base= r2_score(y_train, y_train_base)\n",
    "r2_train_reg = r2_score(y_train, lm.predict(X_train))\n",
    "\n",
    "r2_test_base = r2_score(y_test, y_test_base)\n",
    "r2_test_reg = r2_score(y_test, lm.predict(X_test))\n",
    "print(r2_train_base, r2_train_reg,r2_test_base,r2_test_reg  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring of Null Model\n",
    "- We got a 0 R-squared for our model.  Why 0?  \n",
    "- This is where it is important to understand what R-squared is actually measuring.\n",
    "- On the left side you see the total sum of squared values (ss_tot_train below).  \n",
    "- On the right you see the sum of squares regression (ss_reg_train).\n",
    "- For the null model, the ss_tot_train = ss_reg_train, so R-squared = 0.\n",
    "<br>\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/8/86/Coefficient_of_Determination.svg)\n",
    "- By Orzetto (Own work) [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0) or GFDL (http://www.gnu.org/copyleft/fdl.html)], via Wikimedia Commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total sum of squares \n",
    "ss_tot_train=np.sum((y_train-np.mean(y_train))**2)\n",
    "ss_res_train=np.sum((y_train-lm.predict(X_train))**2)\n",
    "ss_reg_train=np.sum((lm.predict(X_train)-np.mean(y_train))**2)\n",
    "\n",
    "r2_train_reg_manual= 1-(ss_res_train/ss_tot_train)\n",
    "\n",
    "print(r2_train_reg, r2_train_reg_manual, ss_tot_train, ss_res_train, ss_reg_train )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Predict Outcomes\n",
    "- The regression predict uses the trained coefficients and accepts input.  \n",
    "- Here, by passing the origional from boston_df, we can create a new column for the predicted value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df['PRICE_REG']=lm.predict(boston_df.iloc[:,0:13])\n",
    "boston_df[['PRICE', 'PRICE_REG']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Graph Outcomes\n",
    "- Common to grapy predicted vs actual.\n",
    "- Results should show a randomly distributed error function. \n",
    "- Note that there seem to be much larger errors on right side of grant, suggesting something else might be impacting highest values. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter( boston_df['PRICE'], boston_df['PRICE_REG'], s=5 )\n",
    "plt.xlabel( \"Prices\")\n",
    "plt.ylabel( \"Predicted Prices\")\n",
    "plt.title( \"Real vs Predicted Housing Prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make it look pretty with pickle\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "ax = sns.regplot(x=\"PRICE\", y=\"PRICE_REG\", data=boston_df[['PRICE','PRICE_REG']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Graph Residuals\n",
    "- Common to graph predicted - actual (error term).\n",
    "- Results should show a randomly distributed error function. \n",
    "- Here we are showing train and test as different \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "plt.scatter( lm.predict(X_train), lm.predict(X_train) - y_train,\n",
    "           c ='b', s=30, alpha=0.4 )\n",
    "plt.scatter( lm.predict(X_test), lm.predict(X_test) - y_test,\n",
    "           c ='g', s=30 )\n",
    "#The expected error is 0. \n",
    "plt.hlines( y=0, xmin=-5, xmax=55)\n",
    "plt.title( \"Residuals\" )\n",
    "plt.ylabel( \"Residuals\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Persistent Models\n",
    "- I could be that you would want to maintain \n",
    "- The `pickle` package enables storing objects to disk and then retreive them. \n",
    "- For example, for a trained model we might want to store it, and then use it to score additional data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data\n",
    "boston_df.to_csv('../input/boston.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( lm, open( '../input/lm_reg_boston.p', 'wb' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the pickled object. \n",
    "lm_pickled = pickle.load( open( \"../input/lm_reg_boston.p\", \"rb\" ) )\n",
    "\n",
    "lm_pickled.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Copyright [AnalyticsDojo](http://rpi.analyticsdojo.com) 2016.\n",
    "This work is licensed under the [Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/) license agreement.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
