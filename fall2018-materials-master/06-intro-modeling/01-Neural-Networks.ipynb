{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks \n",
    "- This was adopted from the PyTorch Tutorials. \n",
    "- http://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks \n",
    "- Neural networks are the foundation of deep learning, which has revolutionized the \n",
    "\n",
    "```In the mathematical theory of artificial neural networks, the universal approximation theorem states[1] that a feed-forward network with a single hidden layer containing a finite number of neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of Rn, under mild assumptions on the activation function.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Fake Data\n",
    "- `D_in` is the number of dimensions of an input varaible.\n",
    "- `D_out` is the number of dimentions of an output variable.\n",
    "- Here we are learning some special \"fake\" data that represents the xor problem. \n",
    "- Here, the dv is 1 if either the first or second variable is \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]] \n",
      " Output data:\n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "#This is \n",
    "x = np.array([ [0,0,0],[1,0,0],[0,1,0],[0,0,0] ])\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "print(\"Input data:\\n\",X,\"\\n Output data:\\n\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Neural Network \n",
    "- Here we are going to build a neural network with 6 hidden layers. \n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_in, H, D_out = 3, 3, 1\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.520526973721\n",
      "1 0.513053503241\n",
      "2 0.505682867527\n",
      "3 0.498413739025\n",
      "4 0.491244806502\n",
      "5 0.484174774821\n",
      "6 0.477202364718\n",
      "7 0.47032631259\n",
      "8 0.463545370285\n",
      "9 0.45685830489\n",
      "10 0.450263898532\n",
      "11 0.443760948182\n",
      "12 0.437348265457\n",
      "13 0.431024676431\n",
      "14 0.424789021449\n",
      "15 0.418640154944\n",
      "16 0.412576945257\n",
      "17 0.406598274463\n",
      "18 0.400703038197\n",
      "19 0.394890145483\n",
      "20 0.389158518572\n",
      "21 0.383507092775\n",
      "22 0.377934816306\n",
      "23 0.37244065012\n",
      "24 0.367023567765\n",
      "25 0.361682555222\n",
      "26 0.356416610764\n",
      "27 0.351224744802\n",
      "28 0.346105979745\n",
      "29 0.341059349856\n",
      "30 0.336083901114\n",
      "31 0.331178691075\n",
      "32 0.326342788736\n",
      "33 0.321575274403\n",
      "34 0.316875239561\n",
      "35 0.312241786742\n",
      "36 0.3076740294\n",
      "37 0.303171091785\n",
      "38 0.298732108817\n",
      "39 0.294356225971\n",
      "40 0.290042599147\n",
      "41 0.285790394562\n",
      "42 0.281598788624\n",
      "43 0.277466967825\n",
      "44 0.273394128621\n",
      "45 0.269379477323\n",
      "46 0.265422229986\n",
      "47 0.261521612301\n",
      "48 0.257676859485\n",
      "49 0.253887216174\n",
      "50 0.250151936322\n",
      "51 0.246470283094\n",
      "52 0.242841528763\n",
      "53 0.239264954611\n",
      "54 0.235739850827\n",
      "55 0.232265516411\n",
      "56 0.228841259072\n",
      "57 0.225466395134\n",
      "58 0.222140249442\n",
      "59 0.218862155263\n",
      "60 0.215631454198\n",
      "61 0.212447496086\n",
      "62 0.209309638913\n",
      "63 0.206217248724\n",
      "64 0.203169699532\n",
      "65 0.200166373229\n",
      "66 0.1972066595\n",
      "67 0.194289955737\n",
      "68 0.19141566695\n",
      "69 0.188583205686\n",
      "70 0.185791991945\n",
      "71 0.183041453094\n",
      "72 0.180331023787\n",
      "73 0.177660145886\n",
      "74 0.175028268374\n",
      "75 0.172434847283\n",
      "76 0.169879345609\n",
      "77 0.167361233239\n",
      "78 0.164879986867\n",
      "79 0.162435089927\n",
      "80 0.160026032507\n",
      "81 0.157652311282\n",
      "82 0.155313429433\n",
      "83 0.153008896581\n",
      "84 0.150738228707\n",
      "85 0.148500948082\n",
      "86 0.146296583199\n",
      "87 0.144124668698\n",
      "88 0.141984745296\n",
      "89 0.139876359719\n",
      "90 0.137799064636\n",
      "91 0.135752418583\n",
      "92 0.133735985904\n",
      "93 0.131749336678\n",
      "94 0.129792046656\n",
      "95 0.127863697196\n",
      "96 0.125963875194\n",
      "97 0.124092173023\n",
      "98 0.122248188469\n",
      "99 0.120431524668\n",
      "100 0.11864179004\n",
      "101 0.116878598234\n",
      "102 0.115141568057\n",
      "103 0.113430323424\n",
      "104 0.11174449329\n",
      "105 0.110083711591\n",
      "106 0.10844761719\n",
      "107 0.106835853815\n",
      "108 0.105248069998\n",
      "109 0.103683919025\n",
      "110 0.102143058873\n",
      "111 0.100625152156\n",
      "112 0.0991298660713\n",
      "113 0.097656872339\n",
      "114 0.0962058471527\n",
      "115 0.0947764711228\n",
      "116 0.0933684292233\n",
      "117 0.091981410739\n",
      "118 0.0906151092126\n",
      "119 0.0892692223931\n",
      "120 0.087943452184\n",
      "121 0.0866375045924\n",
      "122 0.0853510896786\n",
      "123 0.0840839215059\n",
      "124 0.0828357180916\n",
      "125 0.0816062013573\n",
      "126 0.0803950970811\n",
      "127 0.0792021348492\n",
      "128 0.0780270480086\n",
      "129 0.0768695736198\n",
      "130 0.0757294524106\n",
      "131 0.0746064287299\n",
      "132 0.073500250502\n",
      "133 0.0724106691817\n",
      "134 0.0713374397092\n",
      "135 0.0702803204667\n",
      "136 0.0692390732337\n",
      "137 0.0682134631446\n",
      "138 0.0672032586454\n",
      "139 0.0662082314514\n",
      "140 0.0652281565053\n",
      "141 0.0642628119359\n",
      "142 0.0633119790169\n",
      "143 0.0623754421264\n",
      "144 0.0614529887065\n",
      "145 0.060544409224\n",
      "146 0.0596494971309\n",
      "147 0.0587680488255\n",
      "148 0.0578998636138\n",
      "149 0.0570447436721\n",
      "150 0.0562024940087\n",
      "151 0.0553729224271\n",
      "152 0.0545558394891\n",
      "153 0.0537510584786\n",
      "154 0.0529583953653\n",
      "155 0.0521776687695\n",
      "156 0.0514086999268\n",
      "157 0.0506513126534\n",
      "158 0.0499053333117\n",
      "159 0.049170590776\n",
      "160 0.0484469163995\n",
      "161 0.0477341439808\n",
      "162 0.047032109731\n",
      "163 0.0463406522414\n",
      "164 0.0456596124512\n",
      "165 0.0449888336162\n",
      "166 0.044328161277\n",
      "167 0.0436774432283\n",
      "168 0.0430365294885\n",
      "169 0.0424052722689\n",
      "170 0.0417835259443\n",
      "171 0.0411711470232\n",
      "172 0.0405679941187\n",
      "173 0.0399739279199\n",
      "174 0.0393888111627\n",
      "175 0.0388125086026\n",
      "176 0.0382448869862\n",
      "177 0.0376858150239\n",
      "178 0.0371351633629\n",
      "179 0.0365928045601\n",
      "180 0.0360586130561\n",
      "181 0.0355324651482\n",
      "182 0.0350142389657\n",
      "183 0.0345038144432\n",
      "184 0.0340010732964\n",
      "185 0.0335058989962\n",
      "186 0.0330181767452\n",
      "187 0.0325377934524\n",
      "188 0.03206463771\n",
      "189 0.0315985997691\n",
      "190 0.0311395715167\n",
      "191 0.0306874464527\n",
      "192 0.0302421196666\n",
      "193 0.0298034878155\n",
      "194 0.0293714491015\n",
      "195 0.02894590325\n",
      "196 0.028526751488\n",
      "197 0.0281138965225\n",
      "198 0.0277072425197\n",
      "199 0.027306695084\n",
      "200 0.0269121612373\n",
      "201 0.026523549399\n",
      "202 0.0261407693656\n",
      "203 0.0257637322912\n",
      "204 0.0253923506679\n",
      "205 0.0250265383063\n",
      "206 0.024666210317\n",
      "207 0.0243112830912\n",
      "208 0.0239616742826\n",
      "209 0.0236173027891\n",
      "210 0.0232780887345\n",
      "211 0.0229439534511\n",
      "212 0.0226148194615\n",
      "213 0.0222906104622\n",
      "214 0.0219712513056\n",
      "215 0.0216566679837\n",
      "216 0.0213467876112\n",
      "217 0.0210415384091\n",
      "218 0.0207408496887\n",
      "219 0.0204446518353\n",
      "220 0.0201528762929\n",
      "221 0.0198654555481\n",
      "222 0.0195823231152\n",
      "223 0.0193034135206\n",
      "224 0.0190286622887\n",
      "225 0.0187580059261\n",
      "226 0.0184913819081\n",
      "227 0.0182287286636\n",
      "228 0.0179699855614\n",
      "229 0.0177150928963\n",
      "230 0.0174639918751\n",
      "231 0.0172166246032\n",
      "232 0.0169729340714\n",
      "233 0.0167328641423\n",
      "234 0.0164963595377\n",
      "235 0.0162633658259\n",
      "236 0.0160338294084\n",
      "237 0.0158076975083\n",
      "238 0.0155849181574\n",
      "239 0.0153654401843\n",
      "240 0.0151492132026\n",
      "241 0.0149361875989\n",
      "242 0.0147263145214\n",
      "243 0.0145195458683\n",
      "244 0.0143158342766\n",
      "245 0.0141151331111\n",
      "246 0.0139173964531\n",
      "247 0.0137225790901\n",
      "248 0.0135306365047\n",
      "249 0.0133415248643\n",
      "250 0.0131552010107\n",
      "251 0.0129716224502\n",
      "252 0.0127907473429\n",
      "253 0.0126125344936\n",
      "254 0.0124369433412\n",
      "255 0.01226393395\n",
      "256 0.0120934669992\n",
      "257 0.0119255037744\n",
      "258 0.0117600061581\n",
      "259 0.0115969366203\n",
      "260 0.0114362582101\n",
      "261 0.0112779345465\n",
      "262 0.0111219298098\n",
      "263 0.0109682087332\n",
      "264 0.0108167365943\n",
      "265 0.0106674792063\n",
      "266 0.0105204029108\n",
      "267 0.0103754745689\n",
      "268 0.0102326615533\n",
      "269 0.0100919317411\n",
      "270 0.0099532535053\n",
      "271 0.00981659570779\n",
      "272 0.0096819276915\n",
      "273 0.00954921927316\n",
      "274 0.00941844073597\n",
      "275 0.00928956282251\n",
      "276 0.00916255672759\n",
      "277 0.00903739409136\n",
      "278 0.00891404699242\n",
      "279 0.00879248794107\n",
      "280 0.00867268987271\n",
      "281 0.00855462614122\n",
      "282 0.00843827051254\n",
      "283 0.00832359715833\n",
      "284 0.00821058064967\n",
      "285 0.00809919595093\n",
      "286 0.00798941841368\n",
      "287 0.00788122377067\n",
      "288 0.00777458813001\n",
      "289 0.00766948796929\n",
      "290 0.00756590012993\n",
      "291 0.00746380181147\n",
      "292 0.00736317056611\n",
      "293 0.00726398429317\n",
      "294 0.00716622123375\n",
      "295 0.00706985996542\n",
      "296 0.00697487939701\n",
      "297 0.00688125876346\n",
      "298 0.00678897762074\n",
      "299 0.00669801584092\n",
      "300 0.00660835360719\n",
      "301 0.00651997140908\n",
      "302 0.00643285003767\n",
      "303 0.0063469705809\n",
      "304 0.00626231441897\n",
      "305 0.00617886321979\n",
      "306 0.00609659893449\n",
      "307 0.006015503793\n",
      "308 0.00593556029976\n",
      "309 0.00585675122938\n",
      "310 0.00577905962247\n",
      "311 0.00570246878149\n",
      "312 0.00562696226664\n",
      "313 0.00555252389189\n",
      "314 0.00547913772099\n",
      "315 0.00540678806356\n",
      "316 0.0053354594713\n",
      "317 0.00526513673416\n",
      "318 0.00519580487667\n",
      "319 0.00512744915423\n",
      "320 0.00506005504954\n",
      "321 0.00499360826905\n",
      "322 0.00492809473942\n",
      "323 0.00486350060415\n",
      "324 0.00479981222013\n",
      "325 0.00473701615434\n",
      "326 0.00467509918054\n",
      "327 0.00461404827607\n",
      "328 0.00455385061865\n",
      "329 0.00449449358323\n",
      "330 0.00443596473892\n",
      "331 0.00437825184599\n",
      "332 0.0043213428528\n",
      "333 0.00426522589292\n",
      "334 0.00420988928222\n",
      "335 0.00415532151602\n",
      "336 0.00410151126624\n",
      "337 0.0040484473787\n",
      "338 0.00399611887035\n",
      "339 0.00394451492664\n",
      "340 0.00389362489881\n",
      "341 0.00384343830138\n",
      "342 0.00379394480951\n",
      "343 0.00374513425657\n",
      "344 0.00369699663159\n",
      "345 0.00364952207686\n",
      "346 0.00360270088554\n",
      "347 0.00355652349926\n",
      "348 0.00351098050584\n",
      "349 0.00346606263697\n",
      "350 0.00342176076597\n",
      "351 0.00337806590557\n",
      "352 0.00333496920572\n",
      "353 0.00329246195148\n",
      "354 0.00325053556083\n",
      "355 0.00320918158269\n",
      "356 0.00316839169477\n",
      "357 0.00312815770164\n",
      "358 0.00308847153266\n",
      "359 0.00304932524012\n",
      "360 0.00301071099724\n",
      "361 0.00297262109632\n",
      "362 0.00293504794686\n",
      "363 0.00289798407373\n",
      "364 0.00286142211539\n",
      "365 0.00282535482205\n",
      "366 0.002789775054\n",
      "367 0.00275467577983\n",
      "368 0.00272005007477\n",
      "369 0.00268589111901\n",
      "370 0.00265219219607\n",
      "371 0.00261894669116\n",
      "372 0.00258614808961\n",
      "373 0.00255378997533\n",
      "374 0.00252186602923\n",
      "375 0.00249037002769\n",
      "376 0.00245929584115\n",
      "377 0.00242863743256\n",
      "378 0.00239838885598\n",
      "379 0.00236854425512\n",
      "380 0.00233909786201\n",
      "381 0.00231004399553\n",
      "382 0.00228137706015\n",
      "383 0.00225309154452\n",
      "384 0.00222518202019\n",
      "385 0.00219764314033\n",
      "386 0.00217046963842\n",
      "387 0.00214365632706\n",
      "388 0.00211719809665\n",
      "389 0.00209108991427\n",
      "390 0.00206532682244\n",
      "391 0.00203990393792\n",
      "392 0.00201481645063\n",
      "393 0.00199005962243\n",
      "394 0.00196562878606\n",
      "395 0.00194151934402\n",
      "396 0.00191772676747\n",
      "397 0.00189424659518\n",
      "398 0.00187107443249\n",
      "399 0.00184820595025\n",
      "400 0.00182563688382\n",
      "401 0.00180336303206\n",
      "402 0.00178138025636\n",
      "403 0.00175968447966\n",
      "404 0.00173827168547\n",
      "405 0.001717137917\n",
      "406 0.00169627927616\n",
      "407 0.0016756919227\n",
      "408 0.00165537207329\n",
      "409 0.00163531600064\n",
      "410 0.00161552003268\n",
      "411 0.00159598055161\n",
      "412 0.00157669399315\n",
      "413 0.00155765684568\n",
      "414 0.00153886564942\n",
      "415 0.00152031699564\n",
      "416 0.00150200752586\n",
      "417 0.0014839339311\n",
      "418 0.00146609295109\n",
      "419 0.00144848137353\n",
      "420 0.00143109603334\n",
      "421 0.00141393381195\n",
      "422 0.0013969916366\n",
      "423 0.00138026647957\n",
      "424 0.00136375535755\n",
      "425 0.00134745533094\n",
      "426 0.00133136350317\n",
      "427 0.00131547702003\n",
      "428 0.00129979306905\n",
      "429 0.00128430887883\n",
      "430 0.00126902171844\n",
      "431 0.00125392889674\n",
      "432 0.00123902776186\n",
      "433 0.00122431570052\n",
      "434 0.00120979013748\n",
      "435 0.00119544853493\n",
      "436 0.00118128839196\n",
      "437 0.00116730724394\n",
      "438 0.00115350266203\n",
      "439 0.00113987225256\n",
      "440 0.00112641365656\n",
      "441 0.00111312454919\n",
      "442 0.00110000263923\n",
      "443 0.00108704566858\n",
      "444 0.00107425141173\n",
      "445 0.00106161767529\n",
      "446 0.00104914229748\n",
      "447 0.00103682314766\n",
      "448 0.00102465812587\n",
      "449 0.00101264516234\n",
      "450 0.00100078221702\n",
      "451 0.000989067279173\n",
      "452 0.000977498366909\n",
      "453 0.000966073526734\n",
      "454 0.000954790833136\n",
      "455 0.000943648388155\n",
      "456 0.00093264432097\n",
      "457 0.000921776787486\n",
      "458 0.00091104396993\n",
      "459 0.000900444076453\n",
      "460 0.000889975340741\n",
      "461 0.000879636021627\n",
      "462 0.000869424402713\n",
      "463 0.000859338791998\n",
      "464 0.000849377521509\n",
      "465 0.000839538946939\n",
      "466 0.000829821447292\n",
      "467 0.000820223424533\n",
      "468 0.000810743303242\n",
      "469 0.000801379530274\n",
      "470 0.000792130574429\n",
      "471 0.000782994926117\n",
      "472 0.000773971097038\n",
      "473 0.000765057619862\n",
      "474 0.000756253047915\n",
      "475 0.000747555954873\n",
      "476 0.000738964934453\n",
      "477 0.000730478600116\n",
      "478 0.000722095584776\n",
      "479 0.000713814540504\n",
      "480 0.000705634138245\n",
      "481 0.000697553067538\n",
      "482 0.000689570036238\n",
      "483 0.000681683770243\n",
      "484 0.00067389301323\n",
      "485 0.000666196526383\n",
      "486 0.000658593088143\n",
      "487 0.000651081493943\n",
      "488 0.000643660555963\n",
      "489 0.000636329102877\n",
      "490 0.000629085979615\n",
      "491 0.000621930047116\n",
      "492 0.000614860182094\n",
      "493 0.000607875276807\n",
      "494 0.000600974238826\n",
      "495 0.00059415599081\n",
      "496 0.00058741947028\n",
      "497 0.000580763629409\n",
      "498 0.000574187434795\n",
      "499 0.000567689867259\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "learning_rate = 1e-3\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    \n",
    "    #A relu is just the activation.\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.97891648],\n",
       "       [ 1.01096967],\n",
       "       [ 0.        ]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_pred = h_relu.dot(w2)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.74492939],\n",
       "       [-1.11689861],\n",
       "       [-1.58493737]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.35167184],\n",
       "       [ 0.70907942],\n",
       "       [-0.11154362]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.72108356,  0.        ,  0.        ],\n",
       "       [ 0.72753913,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relu just removes the negative numbers.  \n",
    "h_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
